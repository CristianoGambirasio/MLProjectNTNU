{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: AutoML_Results_TEST/\n",
      "The task is regression with evaluation metric mae\n",
      "AutoML will use algorithms: ['LightGBM', 'Xgboost', 'CatBoost', 'Neural Network']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'mix_encoding', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree: trained.\n",
      "Skip simple_algorithms because no parameters were generated.\n",
      "* Step default_algorithms will try to check up to 4 models\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lasse/Programming/Uni/MLProjectNTNU/AUTOML.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/AUTOML.ipynb#W3sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m cv \u001b[39m=\u001b[39m [(train_idx, val_idx)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/AUTOML.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m mod \u001b[39m=\u001b[39m AutoML(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/AUTOML.ipynb#W3sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     results_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAutoML_Results_TEST/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/AUTOML.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCompete\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/AUTOML.ipynb#W3sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     validation_strategy\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mvalidation_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcustom\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/AUTOML.ipynb#W3sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/AUTOML.ipynb#W3sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m mod\u001b[39m.\u001b[39mfit(X_train_agg, y_train_agg, cv\u001b[39m=\u001b[39mcv)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/supervised/automl.py:433\u001b[0m, in \u001b[0;36mAutoML.fit\u001b[0;34m(self, X, y, sample_weight, cv, sensitive_features)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[1;32m    407\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    408\u001b[0m     X: Union[numpy\u001b[39m.\u001b[39mndarray, pandas\u001b[39m.\u001b[39mDataFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m     ] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m ):\n\u001b[1;32m    416\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the AutoML model.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \n\u001b[1;32m    418\u001b[0m \u001b[39m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m        AutoML object: Returns `self`\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, sample_weight, cv, sensitive_features)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/supervised/base_automl.py:1195\u001b[0m, in \u001b[0;36mBaseAutoML._fit\u001b[0;34m(self, X, y, sample_weight, cv, sensitive_features)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mensemble_step(\n\u001b[1;32m   1192\u001b[0m         is_stacked\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mis_stacked\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1193\u001b[0m     )\n\u001b[1;32m   1194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     trained \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_model(params)\n\u001b[1;32m   1196\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrained\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trained \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mskipped\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1197\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mfinal_loss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_models[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mget_final_loss()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/supervised/base_automl.py:401\u001b[0m, in \u001b[0;36mBaseAutoML.train_model\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m# start training\u001b[39;00m\n\u001b[1;32m    398\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    399\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain model #\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_models)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m / Model name: \u001b[39m\u001b[39m{\u001b[39;00mparams[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m )\n\u001b[0;32m--> 401\u001b[0m mf\u001b[39m.\u001b[39mtrain(results_path, model_subpath)\n\u001b[1;32m    403\u001b[0m \u001b[39m# keep info about the model\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_model(mf, model_subpath)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/supervised/model_framework.py:254\u001b[0m, in \u001b[0;36mModelFramework.train\u001b[0;34m(self, results_path, model_subpath)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(learner\u001b[39m.\u001b[39mmax_iters):\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_iteration_start()\n\u001b[0;32m--> 254\u001b[0m     learner\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    255\u001b[0m         X_train,\n\u001b[1;32m    256\u001b[0m         y_train,\n\u001b[1;32m    257\u001b[0m         sample_weight,\n\u001b[1;32m    258\u001b[0m         X_validation,\n\u001b[1;32m    259\u001b[0m         y_validation,\n\u001b[1;32m    260\u001b[0m         sample_weight_validation,\n\u001b[1;32m    261\u001b[0m         log_to_file,\n\u001b[1;32m    262\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_time_for_learner,\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39minjected_sample_weight\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    266\u001b[0m         \u001b[39m# print(\"Dont use sample weight in model evaluation\")\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         sample_weight \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/supervised/algorithms/lightgbm.py:237\u001b[0m, in \u001b[0;36mLightgbmAlgorithm.fit\u001b[0;34m(self, X, y, sample_weight, X_validation, y_validation, sample_weight_validation, log_to_file, max_time)\u001b[0m\n\u001b[1;32m    232\u001b[0m evals_result \u001b[39m=\u001b[39m {}\n\u001b[1;32m    234\u001b[0m \u001b[39m# disable for now ...\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m# boosting_rounds = self.get_boosting_rounds(lgb_train, valid_sets, esr, max_time)\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m    238\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearner_params,\n\u001b[1;32m    239\u001b[0m     lgb_train,\n\u001b[1;32m    240\u001b[0m     num_boost_round\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrounds,\n\u001b[1;32m    241\u001b[0m     valid_sets\u001b[39m=\u001b[39mvalid_sets,\n\u001b[1;32m    242\u001b[0m     valid_names\u001b[39m=\u001b[39mvalid_names,\n\u001b[1;32m    243\u001b[0m     feval\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_eval_metric,\n\u001b[1;32m    244\u001b[0m     callbacks\u001b[39m=\u001b[39m[\n\u001b[1;32m    245\u001b[0m         lgb\u001b[39m.\u001b[39mearly_stopping(esr, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m    246\u001b[0m         lgb\u001b[39m.\u001b[39mrecord_evaluation(evals_result),\n\u001b[1;32m    247\u001b[0m     ],\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[39mdel\u001b[39;00m lgb_train\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m valid_sets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39mupdate(fobj\u001b[39m=\u001b[39mfobj)\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39mbyref(is_finished)))\n\u001b[1;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "algos = ['LightGBM', 'Xgboost', 'CatBoost', 'Neural Network']\n",
    "\n",
    "from supervised.automl import AutoML\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "training_filters = [\n",
    "    {\n",
    "        'feature': 'month',\n",
    "        'operator': 'between',\n",
    "        'value': [3, 9]\n",
    "    }\n",
    "]\n",
    "\n",
    "validation_filters = [\n",
    "    {\n",
    "        'feature': 'date_forecast',\n",
    "        'operator': 'between',\n",
    "        'value': ['2020-05-01', '2020-07-01']\n",
    "    }\n",
    "]\n",
    "\n",
    "test_filters = [\n",
    "    {\n",
    "        'feature': 'date_forecast',\n",
    "        'operator': 'between',\n",
    "        'value': ['2021-05-01', '2021-07-01']\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "agg_strats = ['min','max','mean','std']\n",
    "\n",
    "m_wrap = DataWrapper(\n",
    "    impute_strategy = 'fbfill',\n",
    "    aggregation_strats = agg_strats,\n",
    "    y_scalers = [Y_Scaler_MaxAbs_per_building()],\n",
    "    transform_pred_strategy = [],\n",
    "    training_filters=training_filters,\n",
    "    validation_filters=validation_filters,\n",
    "    test_filters=test_filters,\n",
    "    bagging_filters=False, # these are applied additionally to the other filters on val, test, sub\n",
    "    )\n",
    "\n",
    "X_train_agg, y_train_agg = m_wrap.get_train(aggregated=True,drop_val=False,drop_test=False,y_scaled=True)\n",
    "X_sub_agg = m_wrap.get_sub(aggregated=True)\n",
    "\n",
    "train_idx = X_train_agg[~X_train_agg['date_forecast'].between('2020-05-01', '2020-07-01')].index\n",
    "val_idx = X_train_agg[X_train_agg['date_forecast'].between('2020-05-01', '2020-07-01')].index\n",
    "\n",
    "X_train_agg = X_train_agg.select_dtypes(exclude=['datetime','timedelta','period[M]'])\n",
    "\n",
    "cv = [(train_idx, val_idx)]\n",
    "\n",
    "mod = AutoML(\n",
    "    results_path='AutoML_Results_TEST/',\n",
    "    mode='Compete',\n",
    "    total_time_limit=60*60*8,\n",
    "    eval_metric='mae',\n",
    "    algorithms=algos,\n",
    "    train_ensemble=True,\n",
    "    stack_models=True,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    validation_strategy={\"validation_type\": \"custom\"}\n",
    ")\n",
    "\n",
    "mod.fit(X_train_agg, y_train_agg, cv=cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
