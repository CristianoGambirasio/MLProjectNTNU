{"cells":[{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor,ExtraTreesRegressor, VotingRegressor, StackingRegressor\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_selection import RFECV, RFE\n","import xgboost\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from skopt.space import Real, Categorical, Integer\n","from lightgbm import LGBMRegressor\n","\n","from skopt import BayesSearchCV\n","\n","train_a = pd.read_parquet('A/train_targets.parquet')\n","train_b = pd.read_parquet('B/train_targets.parquet')\n","train_c = pd.read_parquet('C/train_targets.parquet')\n","\n","X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n","X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n","X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n","\n","X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n","X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n","X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n","\n","X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n","X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n","X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\n","\n","\n","train_a['building_id'] = 'a'\n","train_b['building_id'] = 'b'\n","train_c['building_id'] = 'c'\n","\n","X_train_estimated_a['building_id'] = 'a'\n","X_train_estimated_b['building_id'] = 'b'\n","X_train_estimated_c['building_id'] = 'c'\n","\n","X_train_observed_a['building_id'] = 'a'\n","X_train_observed_b['building_id'] = 'b'\n","X_train_observed_c['building_id'] = 'c'\n","\n","X_test_estimated_a['building_id'] = 'a'\n","X_test_estimated_b['building_id'] = 'b'\n","X_test_estimated_c['building_id'] = 'c'\n","\n","X_test = pd.concat([X_test_estimated_a, X_test_estimated_b, X_test_estimated_c])\n","X_test['time'] = X_test['date_forecast'].dt.floor('H')\n","X_test = X_test.groupby(['building_id', 'time']).mean().reset_index()\n","\n","X_test_delta = (X_test['time']-X_test['date_calc']).apply(lambda x: x.total_seconds() / 3600)\n","\n","\n","X_test.drop(['date_calc'], axis=1, inplace=True)\n","\n","X_observed = pd.concat([X_train_observed_a, X_train_observed_b, X_train_observed_c])\n","X_observed['time'] = X_observed['date_forecast'].dt.floor('H')\n","\n","y_train = pd.concat([train_a, train_b, train_c])\n","\n","X_observed_grouped = X_observed.groupby(['building_id', 'time']).mean()\n","\n","# combine and remove rows with missing values in y\n","Xy_observed = pd.merge(X_observed_grouped, y_train, on=['time', 'building_id'], how='inner')\n","Xy_observed = Xy_observed[Xy_observed['pv_measurement'].notna()]\n","X = Xy_observed.drop(['pv_measurement'], axis=1)\n","y = Xy_observed['pv_measurement']\n","\n","X['time_month'] = X['time'].dt.month\n","X_test['time_month'] = X_test['time'].dt.month\n","\n","drop_cols = ['time', 'date_forecast', 'snow_density:kgm3','clear_sky_energy_1h:J',\n"," 'direct_rad_1h:J',\n"," 'fresh_snow_24h:cm',\n"," 'fresh_snow_1h:cm',\n"," 'fresh_snow_12h:cm',\n"," 'diffuse_rad_1h:J',\n"," 'dew_point_2m:K',\n"," 'dew_or_rime:idx',\n"," 'precip_5min:mm',\n"," 'fresh_snow_6h:cm',\n"," 'prob_rime:p',\n"," 'ceiling_height_agl:m',\n"," 'rain_water:kgm2',\n"," 'sfc_pressure:hPa',\n"," 'snow_depth:cm',\n"," 'snow_drift:idx',\n"," 'snow_melt_10min:mm',\n"," 'snow_water:kgm2',\n"," 'pressure_50m:hPa',\n"," 'wind_speed_w_1000hPa:ms',\n"," 'pressure_100m:hPa',\n"," 'fresh_snow_3h:cm']\n","X = X.drop(drop_cols, axis=1)\n","X_test = X_test.drop(drop_cols,axis=1)\n","# get y mean per building id\n","mean_y_per_building = y.groupby(Xy_observed['building_id']).mean()\n","\n","# divide y by mean per building id\n","y = y.groupby(Xy_observed['building_id']).transform(lambda x: x / mean_y_per_building[x.name]) \n","\n","# setting types of columns\n","categorical_features = [\n","    'building_id'\n","]\n","\n","impute_features = [\n","    \"cloud_base_agl:m\",\n","    #\"ceiling_height_agl:m\",\n","]\n"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["LGB = LGBMRegressor(\n","    colsample_bytree = 0.5,\n","    learning_rate = 0.1,\n","    max_depth = 11,\n","    min_child_samples = 20,\n","    min_child_weight = 10,\n","    n_estimators = 100,\n","    num_leaves = 100,\n","    reg_alpha = 0.57,\n","    reg_lambda = 0.45,\n","    subsample = 0.9\n",")\n","\n","XT = ExtraTreesRegressor(\n","    max_depth=10,\n","    min_samples_split= 3,\n","    n_estimators=173\n",")\n","\n","XGB = xgboost.XGBRegressor(\n","    colsample_bytree = 0.5,\n","    gamma = 0,\n","    learning_rate = 0.07,\n","    max_depth = 10,\n","    n_estimators = 174,\n","    reg_alpha = 0,\n","    reg_lambda = 5,\n","    subsample = 1\n",")\n","\n","HGB = HistGradientBoostingRegressor(\n","    learning_rate=0.04,\n","    max_depth=10,\n","    max_iter=1000,\n","    min_samples_leaf=20,\n",")"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["estimators=[('lgbgf',LGB),('xgb',XGB),('hgb',HGB)]\n","\n","Vote=VotingRegressor([('lgbgf',LGB),('xgb',XGB),('hgb',HGB)])\n","\n","Stack=StackingRegressor(estimators=estimators)"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003699 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 4420\n","[LightGBM] [Info] Number of data points in the train set: 82026, number of used features: 26\n","[LightGBM] [Info] Start training from score 1.000000\n"]}],"source":["# set column transformer\n","columnTransformer = ColumnTransformer(\n","    transformers=[\n","        ('imputer', SimpleImputer(strategy='mean'),impute_features),\n","        ('oneHotEncoder', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n","    ],\n","    remainder='passthrough',  # Include other columns\n","    n_jobs=-1\n",")\n","\n","# build the pipeline\n","pipeline = Pipeline(steps=[\n","    ('columnTransformer', columnTransformer),\n","    ('estimator', Vote)\n","])\n","\n","vote_model1 = pipeline.fit(X,y)\n"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["X_train_estimated = pd.concat([X_train_estimated_a, X_train_estimated_b, X_train_estimated_c])\n","\n","X_train_estimated['delta_forecasting'] = (X_train_estimated['date_forecast']-X_train_estimated['date_calc']).apply(lambda x: x.total_seconds() / 3600)\n","X_test['delta_forecasting'] = X_test_delta\n","\n","X_train_estimated['time'] = X_train_estimated['date_forecast'].dt.floor('H')\n","X_train_estimated = X_train_estimated.groupby(['building_id', 'time']).mean().reset_index()\n","\n","Xy_estimated = pd.merge(X_train_estimated, y_train, on=['time', 'building_id'], how='inner')\n","Xy_estimated = Xy_estimated[Xy_estimated['pv_measurement'].notna()]\n","\n","Xe = Xy_estimated.drop(['pv_measurement'], axis=1)\n","ye = Xy_estimated['pv_measurement']\n","Xe['time_month'] = Xe['time'].dt.month\n","\n","ye = ye.groupby(Xy_estimated['building_id']).transform(lambda x: x / mean_y_per_building[x.name]) \n","\n","drop_cols = ['time', 'date_forecast', 'snow_density:kgm3','date_calc','clear_sky_energy_1h:J',\n"," 'direct_rad_1h:J',\n"," 'fresh_snow_24h:cm',\n"," 'fresh_snow_1h:cm',\n"," 'fresh_snow_12h:cm',\n"," 'diffuse_rad_1h:J',\n"," 'dew_point_2m:K',\n"," 'dew_or_rime:idx',\n"," 'precip_5min:mm',\n"," 'fresh_snow_6h:cm',\n"," 'prob_rime:p',\n"," 'ceiling_height_agl:m',\n"," 'rain_water:kgm2',\n"," 'sfc_pressure:hPa',\n"," 'snow_depth:cm',\n"," 'snow_drift:idx',\n"," 'snow_melt_10min:mm',\n"," 'snow_water:kgm2',\n"," 'pressure_50m:hPa',\n"," 'wind_speed_w_1000hPa:ms',\n"," 'pressure_100m:hPa',\n"," 'fresh_snow_3h:cm']\n","Xe = Xe.drop(drop_cols, axis=1)"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["y_pred_e = vote_model1.predict(Xe)\n","\n","y_pred_test = vote_model1.predict(X_test)\n","\n","\n"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["Xe['first_pred'] = y_pred_e\n","X_test['first_pred'] = y_pred_test"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["rf = RandomForestRegressor()\n","\n","columnTransformer2 = ColumnTransformer(\n","    transformers=[\n","        ('imputer', SimpleImputer(strategy='mean'),impute_features),\n","        ('oneHotEncoder', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n","    ],\n","    remainder='passthrough',  # Include other columns\n","    n_jobs=-1\n",")\n","\n","# build the pipeline\n","pipeline2 = Pipeline(steps=[\n","    ('columnTransformer', columnTransformer),\n","    ('estimator', rf)\n","])\n","\n","second_model = pipeline2.fit(Xe,ye)"]}],"metadata":{"kernelspec":{"display_name":"ML-NTNU","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":2}
