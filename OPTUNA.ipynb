{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "import xgboost\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from helpers import *\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# auto reloading library (mainly for altering helpers.py)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['time', 'date_forecast', 'snow_density:kgm3', 'date_calc', 'monthYear', 'dayMonthYear']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 09:32:26,636] A new study created in memory with name: no-name-d7c72334-dbc1-4cb5-bdea-7a66762ce2a6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 09:32:45,537] Trial 0 finished with value: -2.273872243769831 and parameters: {'shuffle': True, 'n_estimators': 600}. Best is trial 0 with value: -2.273872243769831.\n",
      "[I 2023-11-02 09:33:00,432] Trial 1 finished with value: -2.1234206030627414 and parameters: {'shuffle': True, 'n_estimators': 500}. Best is trial 1 with value: -2.1234206030627414.\n",
      "[I 2023-11-02 09:33:15,099] Trial 2 finished with value: 0.7295621726345344 and parameters: {'shuffle': False, 'n_estimators': 500}. Best is trial 2 with value: 0.7295621726345344.\n",
      "[I 2023-11-02 09:33:33,819] Trial 3 finished with value: 0.706877614107545 and parameters: {'shuffle': False, 'n_estimators': 700}. Best is trial 2 with value: 0.7295621726345344.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  4\n",
      "Best trial:\n",
      "  Value: 0.7295621726345344\n",
      "  Params: \n",
      "    shuffle: False\n",
      "    n_estimators: 500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optuna example that optimizes a classifier configuration for cancer dataset\n",
    "using XGBoost.\n",
    "\n",
    "In this example, we optimize the validation accuracy of cancer detection\n",
    "using XGBoost. We optimize both the choice of booster model and its\n",
    "hyperparameters.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def objective(trial):\n",
    "    # Load data\n",
    "    Xy_train, _ = get_splitted_data()\n",
    "\n",
    "    # Add features\n",
    "    Xy_train = add_features(Xy_train.copy())\n",
    "\n",
    "    X_train, y_train = split_Xy_X_y(Xy_train)\n",
    "\n",
    "    # shuffle data\n",
    "    if trial.suggest_categorical('shuffle', [True, False]):\n",
    "        X_train = X_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # drop columns\n",
    "    X_train = X_train.drop(columns=drop_cols,errors='ignore')\n",
    "\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    impute_features = X_train.loc[:, X_train.isna().any()].columns.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    # set column transformer\n",
    "    columnTransformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('imputer', SimpleImputer(strategy='constant'),impute_features),\n",
    "            ('oneHotEncoder', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ],\n",
    "        remainder='passthrough',  # Dont drop remaining columns\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # build the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('columnTransformer', columnTransformer),\n",
    "        ('statusSaver', StatusSaver()),\n",
    "        ('estimator', xgboost.XGBRegressor(\n",
    "            random_state=42,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            reg_alpha=8,\n",
    "            reg_lambda=5,\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 1000, 100),\n",
    "            colsample_bytree=1,\n",
    "            min_child_weight=3,\n",
    "            ))\n",
    "    ])\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, error_score='raise')\n",
    "    return scores.mean()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100, timeout=60)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#X62sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# predict on estimated data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#X62sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m m1_pred \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(m1\u001b[39m.\u001b[39mpredict(X_test))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#X62sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m t\u001b[39m=\u001b[39mm1_pred\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#X62sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#m1_pred = pd.Series(full_scaler.inverse_transform(m1_pred.values.reshape(-1, 1)).flatten())\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm1' is not defined"
     ]
    }
   ],
   "source": [
    "# predict on estimated data\n",
    "m1_pred = pd.Series(m1.predict(X_test))\n",
    "t=m1_pred.copy()\n",
    "#m1_pred = pd.Series(full_scaler.inverse_transform(m1_pred.values.reshape(-1, 1)).flatten())\n",
    "m1_pred = y_scaler.inverse_transform(m1_pred, X_test['building_id'])\n",
    "Xy_test['m1_pred'] = m1_pred\n",
    "\n",
    "# calculate abs diff\n",
    "Xy_test['abs_diff'] = np.abs(Xy_test['pv_measurement'] - Xy_test['m1_pred'])\n",
    "Xy_test['diff'] = (Xy_test['pv_measurement'] - Xy_test['m1_pred'])\n",
    "\n",
    "# calculate mae\n",
    "mae = Xy_test['abs_diff'].mean()\n",
    "print('MAE:', mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=Xy_test, x='time', y='pv_measurement', hue='building_id', legend=False)\n",
    "plt.xticks(rotation=90);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=Xy_test, x='time', y='diff', hue='building_id', legend=False)\n",
    "plt.xticks(rotation=90);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the submission file\n",
    "m1.fit(X, y)\n",
    "\n",
    "# prepare dataframes\n",
    "y_test_pred = pd.Series(m1.predict(X_submission))\n",
    "# y_test_pred = pd.Series(full_scaler.inverse_transform(\n",
    "#     y_test_pred.values.reshape(-1, 1)).flatten())\n",
    "#y_test_pred = y_scaler.inverse_transform(y_test_pred, X_t['building_id']).copy()\n",
    "\n",
    "# remove negative predictions\n",
    "y_test_pred.iloc[y_test_pred < 0] = 0\n",
    "\n",
    "# rename columns etc.\n",
    "y_test_pred = y_test_pred.reset_index().rename(\n",
    "    columns={'pv_measurement': 'prediction', 'index': 'id'})\n",
    "\n",
    "# save submission file\n",
    "y_test_pred.to_csv(\n",
    "    'feature_extraction.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-NTNU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
