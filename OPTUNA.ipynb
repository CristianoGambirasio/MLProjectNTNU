{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "import xgboost\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from helpers import *\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# auto reloading library (mainly for altering helpers.py)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_largeX_smallY(X,y,merge_type='mean'):\n",
    "    # Add time column that only holds the hour\n",
    "    X['time'] = X['date_forecast'].dt.floor('H')\n",
    "\n",
    "    \n",
    "    if merge_type == 'mean':\n",
    "        # Perform Transformation from 4 to 1 values per hour\n",
    "        X = X.groupby(['building_id', 'time']).mean().reset_index()\n",
    "        y = y\n",
    "    else:\n",
    "        raise ValueError(f'merge_type \\'{merge_type}\\' not supported')\n",
    "    \n",
    "    # Merge X and y\n",
    "    Xy = pd.merge(X, y, on=['building_id', 'time']).reset_index(drop=True)\n",
    "    # Drop the time column\n",
    "    Xy = Xy.drop(columns=['time'])\n",
    "    return Xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # Read data\n",
    "    train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "    X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "    X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "    X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "    X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "    X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "    X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "    X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\n",
    "\n",
    "    # Add building_id\n",
    "    train_a['building_id'] = 'a'\n",
    "    train_b['building_id'] = 'b'\n",
    "    train_c['building_id'] = 'c'\n",
    "\n",
    "    X_train_estimated_a['building_id'] = 'a'\n",
    "    X_train_estimated_b['building_id'] = 'b'\n",
    "    X_train_estimated_c['building_id'] = 'c'\n",
    "\n",
    "    X_train_observed_a['building_id'] = 'a'\n",
    "    X_train_observed_b['building_id'] = 'b'\n",
    "    X_train_observed_c['building_id'] = 'c'\n",
    "\n",
    "    X_test_estimated_a['building_id'] = 'a'\n",
    "    X_test_estimated_b['building_id'] = 'b'\n",
    "    X_test_estimated_c['building_id'] = 'c'\n",
    "\n",
    "    # Combine Data\n",
    "    X_o = pd.concat([X_train_observed_a, X_train_observed_b, X_train_observed_c])\n",
    "    X_e = pd.concat([X_train_estimated_a, X_train_estimated_b, X_train_estimated_c])\n",
    "    X_submission = pd.concat([X_test_estimated_a, X_test_estimated_b, X_test_estimated_c])\n",
    "    y = pd.concat([train_a, train_b, train_c])\n",
    "\n",
    "    # Add isEstimated column\n",
    "    X_o['isEstimated'] = 0\n",
    "    X_e['isEstimated'] = 1\n",
    "    X_submission['isEstimated'] = 1\n",
    "\n",
    "\n",
    "    # Combine\n",
    "    X = pd.concat([X_o, X_e])\n",
    "\n",
    "    return X, y, X_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitted_data(merge_type='mean', split_strategy='2021_summer'):\n",
    "    X, y, X_submission = get_data()\n",
    "    # Merge X and y\n",
    "    Xy = merge_largeX_smallY(X,y, merge_type=merge_type)\n",
    "\n",
    "    # Split into train and test\n",
    "    if split_strategy == '2021_summer':\n",
    "        val_idx = Xy.date_forecast.between('2021-05-01', '2021-07-31')\n",
    "    else:\n",
    "        raise ValueError(f'split_strategy \\'{split_strategy}\\' not supported')\n",
    "\n",
    "    Xy_train = Xy[~val_idx]\n",
    "    Xy_val = Xy[val_idx]\n",
    "\n",
    "\n",
    "    # Split into X and y\n",
    "    X_train = Xy_train.drop(['pv_measurement'], axis=1)\n",
    "    y_train = Xy_train['pv_measurement']\n",
    "\n",
    "    X_val = Xy_val.drop(['pv_measurement'], axis=1)\n",
    "    y_val = Xy_val['pv_measurement']\n",
    "\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_time(X):\n",
    "    # Add monthYear column\n",
    "    X['monthYear'] = X['date_forecast'].dt.to_period('M')\n",
    "\n",
    "    # Add dayMonthYear column\n",
    "    X['dayMonthYear'] = X['date_forecast'].dt.to_period('D')\n",
    "\n",
    "    # Add month column\n",
    "    X['month'] = X['date_forecast'].dt.month\n",
    "\n",
    "    # Add week column\n",
    "    #X['week'] = X['date_forecast'].dt.week\n",
    "\n",
    "    # Add day column\n",
    "    X['day'] = X['date_forecast'].dt.day\n",
    "\n",
    "    # Add hour column\n",
    "    X['hour'] = X['date_forecast'].dt.hour\n",
    "\n",
    "    # Calculate difference between date_forecast and date_calc\n",
    "    X['delta_forecast'] = (X['date_forecast']-X['date_calc']).apply(lambda x: x.total_seconds() / 3600)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_general(X, norm_radiation_cap=None):\n",
    "    max_val = 1\n",
    "\n",
    "    X['GHI'] = X['diffuse_rad:W'] + X['direct_rad:W']\n",
    "    X['wind_angle'] = np.arctan2(X['wind_speed_u_10m:ms'], X['wind_speed_v_10m:ms'])\n",
    "    X['norm_radiation'] = (X['GHI'] / X['clear_sky_rad:W']).fillna(0)\n",
    "    if norm_radiation_cap:\n",
    "        X.loc[X['norm_radiation'] > norm_radiation_cap, 'norm_radiation'] = norm_radiation_cap\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_lag(X, na_fill_value=None):\n",
    "    # lagged feature of GHI\n",
    "    X['GHI_lag-2'] = X.groupby('building_id')['GHI'].shift(-2)\n",
    "    X['GHI_lag-1'] = X.groupby('building_id')['GHI'].shift(-1)\n",
    "    X['GHI_lag1'] = X.groupby('building_id')['GHI'].shift(1)\n",
    "    X['GHI_lag2'] = X.groupby('building_id')['GHI'].shift(2)\n",
    "\n",
    "    if na_fill_value is None:\n",
    "        X = X.dropna(subset=['GHI_lag-2', 'GHI_lag-1', 'GHI_lag1', 'GHI_lag2'])\n",
    "    else:\n",
    "        print('WADDEHADDEDUDEDA')\n",
    "        print(na_fill_value)\n",
    "        X = X.fillna(na_fill_value)\n",
    "\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_interaction(X):\n",
    "    # general, somewhat intuitive interactions\n",
    "    X['temp*GHI'] = X['GHI'] * X['t_1000hPa:K']\n",
    "    X['wind*humidity'] = X['wind_speed_10m:ms'] * X['relative_humidity_1000hPa:p']\n",
    "    X['sun_height*diff_rad'] = X['sun_elevation:d'] * X['diffuse_rad:W']\n",
    "\n",
    "    # chat GPT\n",
    "    X['hour*wind_speed_10m'] = X['hour'] * X['wind_speed_10m:ms']\n",
    "    X['hour*clear_sky_rad'] = X['hour'] * X['clear_sky_rad:W']\n",
    "    X['month*sun_elevation'] = X['month'] * X['sun_elevation:d']\n",
    "\n",
    "    X['relative_humidity*air_density'] = X['relative_humidity_1000hPa:p'] * X['air_density_2m:kgm3']\n",
    "    X['temperature*wind_speed'] = X['t_1000hPa:K'] * X['wind_speed_10m:ms']\n",
    "\n",
    "    X['GHI*clear_sky_energy'] = X['GHI'] * X['clear_sky_energy_1h:J']\n",
    "    X['GHI*sun_azimuth'] = X['GHI'] * X['sun_azimuth:d']\n",
    "\n",
    "    X['snow_depth*temp*GHI'] = X['snow_depth:cm'] * X['temp*GHI']\n",
    "\n",
    "    X['GHI_lag_interaction'] = X['GHI_lag-1'] * X['GHI_lag-2'] * X['GHI_lag1'] * X['GHI_lag2']\n",
    "    X['GHI_lag_interaction_all'] = X['GHI_lag-1'] * X['GHI_lag-2'] * X['GHI_lag1'] * X['GHI_lag2'] * X['GHI']\n",
    "\n",
    "    X['wind_speed*temp*GHI'] = X['wind_speed_10m:ms'] * X['t_1000hPa:K'] * X['GHI']\n",
    "\n",
    "    X['cloud_base*clear_sky_energy'] = X['cloud_base_agl:m'] * X['clear_sky_energy_1h:J']\n",
    "\n",
    "    X['precip*sun_elevation'] = X['precip_5min:mm'] * X['sun_elevation:d']\n",
    "    X['supercooled_water*wind_speed'] = X['super_cooled_liquid_water:kgm2'] * X['wind_speed_10m:ms']\n",
    "\n",
    "    return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_mean(X):\n",
    "    X['GHI_mean'] = X.groupby(['building_id', 'dayMonthYear'])['GHI'].transform('mean')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_differences(X):\n",
    "    X['GHI_0_minus_-1'] = X['GHI'] - X['GHI_lag-1']\n",
    "    X['GHI_0_minus_-2'] = X['GHI'] - X['GHI_lag-2']\n",
    "    X['GHI_0_minus_1'] = X['GHI'] - X['GHI_lag1']\n",
    "    X['GHI_0_minus_2'] = X['GHI'] - X['GHI_lag2']\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how to handle function parameters? have to set them for test and submission\n",
    "\n",
    "def add_features(X):\n",
    "    X = add_features_time(X)\n",
    "    X = add_features_general(X)\n",
    "    X = add_features_lag(X)\n",
    "    X = add_features_interaction(X)\n",
    "    X = add_features_mean(X)\n",
    "    X = add_features_differences(X)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['time', 'date_forecast', 'snow_density:kgm3', 'date_calc', 'monthYear', 'dayMonthYear']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-02 02:29:02,497] A new study created in memory with name: no-name-296af20a-2474-47be-a907-ad638b9acfc1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['temp*GHI'] = X['GHI'] * X['t_1000hPa:K']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['wind*humidity'] = X['wind_speed_10m:ms'] * X['relative_humidity_1000hPa:p']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['sun_height*diff_rad'] = X['sun_elevation:d'] * X['diffuse_rad:W']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['hour*wind_speed_10m'] = X['hour'] * X['wind_speed_10m:ms']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['hour*clear_sky_rad'] = X['hour'] * X['clear_sky_rad:W']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['month*sun_elevation'] = X['month'] * X['sun_elevation:d']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['relative_humidity*air_density'] = X['relative_humidity_1000hPa:p'] * X['air_density_2m:kgm3']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['temperature*wind_speed'] = X['t_1000hPa:K'] * X['wind_speed_10m:ms']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['GHI*clear_sky_energy'] = X['GHI'] * X['clear_sky_energy_1h:J']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['GHI*sun_azimuth'] = X['GHI'] * X['sun_azimuth:d']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['snow_depth*temp*GHI'] = X['snow_depth:cm'] * X['temp*GHI']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['GHI_lag_interaction'] = X['GHI_lag-1'] * X['GHI_lag-2'] * X['GHI_lag1'] * X['GHI_lag2']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['GHI_lag_interaction_all'] = X['GHI_lag-1'] * X['GHI_lag-2'] * X['GHI_lag1'] * X['GHI_lag2'] * X['GHI']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['wind_speed*temp*GHI'] = X['wind_speed_10m:ms'] * X['t_1000hPa:K'] * X['GHI']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['cloud_base*clear_sky_energy'] = X['cloud_base_agl:m'] * X['clear_sky_energy_1h:J']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['precip*sun_elevation'] = X['precip_5min:mm'] * X['sun_elevation:d']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/3772693633.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['supercooled_water*wind_speed'] = X['super_cooled_liquid_water:kgm2'] * X['wind_speed_10m:ms']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/2847628283.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['GHI_mean'] = X.groupby(['building_id', 'dayMonthYear'])['GHI'].transform('mean')\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/2563246132.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['GHI_0_minus_-1'] = X['GHI'] - X['GHI_lag-1']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/2563246132.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['GHI_0_minus_-2'] = X['GHI'] - X['GHI_lag-2']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/2563246132.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['GHI_0_minus_1'] = X['GHI'] - X['GHI_lag1']\n",
      "/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/2563246132.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['GHI_0_minus_2'] = X['GHI'] - X['GHI_lag2']\n",
      "[W 2023-11-02 02:29:02,916] Trial 0 failed with parameters: {'shuffle': False, 'n_estimators': 100} because of the following error: ValueError('Found input variables with inconsistent numbers of samples: [92450, 92462]').\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/s_/tyyjfphn3wl93jz8bhkrh3x40000gn/T/ipykernel_82483/2707414924.py\", line 56, in objective\n",
      "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 515, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 252, in cross_validate\n",
      "    X, y, groups = indexable(X, y, groups)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 443, in indexable\n",
      "    check_consistent_length(*result)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 397, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [92450, 92462]\n",
      "[W 2023-11-02 02:29:02,919] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [92450, 92462]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, timeout\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNumber of finished trials: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(study\u001b[39m.\u001b[39mtrials))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# build the pipeline\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(steps\u001b[39m=\u001b[39m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mcolumnTransformer\u001b[39m\u001b[39m'\u001b[39m, columnTransformer),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mstatusSaver\u001b[39m\u001b[39m'\u001b[39m, StatusSaver()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         ))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m ])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m scores \u001b[39m=\u001b[39m cross_val_score(pipeline, X_train, y_train, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lasse/Programming/Uni/MLProjectNTNU/OPTUNA.ipynb#Y111sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mreturn\u001b[39;00m scores\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:252\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcross_validate\u001b[39m(\n\u001b[1;32m     50\u001b[0m     estimator,\n\u001b[1;32m     51\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     error_score\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan,\n\u001b[1;32m     64\u001b[0m ):\n\u001b[1;32m     65\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m    [0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    254\u001b[0m     cv \u001b[39m=\u001b[39m check_cv(cv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    256\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(scoring):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/ML/lib/python3.11/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [92450, 92462]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optuna example that optimizes a classifier configuration for cancer dataset\n",
    "using XGBoost.\n",
    "\n",
    "In this example, we optimize the validation accuracy of cancer detection\n",
    "using XGBoost. We optimize both the choice of booster model and its\n",
    "hyperparameters.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def objective(trial):\n",
    "    # Load data\n",
    "    X_train, y_train, _, _ = get_splitted_data()\n",
    "\n",
    "    # Add features\n",
    "    X_train = add_features(X_train)\n",
    "\n",
    "    # shuffle data\n",
    "    if trial.suggest_categorical('shuffle', [True, False]):\n",
    "        X_train = X_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # drop columns\n",
    "    X_train = X_train.drop(columns=drop_cols,errors='ignore')\n",
    "\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    impute_features = X_train.loc[:, X_train.isna().any()].columns.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    # set column transformer\n",
    "    columnTransformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('imputer', SimpleImputer(strategy='constant'),impute_features),\n",
    "            ('oneHotEncoder', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ],\n",
    "        remainder='passthrough',  # Dont drop remaining columns\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # build the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('columnTransformer', columnTransformer),\n",
    "        ('statusSaver', StatusSaver()),\n",
    "        ('estimator', xgboost.XGBRegressor(\n",
    "            random_state=42,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            reg_alpha=8,\n",
    "            reg_lambda=5,\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 1000, 100),\n",
    "            colsample_bytree=1,\n",
    "            min_child_weight=3,\n",
    "            ))\n",
    "    ])\n",
    "\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "    return scores.mean()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=100, timeout=60)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding building ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a['building_id'] = 'a'\n",
    "train_b['building_id'] = 'b'\n",
    "train_c['building_id'] = 'c'\n",
    "\n",
    "X_train_estimated_a['building_id'] = 'a'\n",
    "X_train_estimated_b['building_id'] = 'b'\n",
    "X_train_estimated_c['building_id'] = 'c'\n",
    "\n",
    "X_train_observed_a['building_id'] = 'a'\n",
    "X_train_observed_b['building_id'] = 'b'\n",
    "X_train_observed_c['building_id'] = 'c'\n",
    "\n",
    "X_test_estimated_a['building_id'] = 'a'\n",
    "X_test_estimated_b['building_id'] = 'b'\n",
    "X_test_estimated_c['building_id'] = 'c'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Data\n",
    "X_o = pd.concat([X_train_observed_a, X_train_observed_b, X_train_observed_c])\n",
    "X_e = pd.concat([X_train_estimated_a, X_train_estimated_b, X_train_estimated_c])\n",
    "X_submission = pd.concat([X_test_estimated_a, X_test_estimated_b, X_test_estimated_c])\n",
    "y = pd.concat([train_a, train_b, train_c])\n",
    "\n",
    "# Add isEstimated column\n",
    "X_o['isEstimated'] = 0\n",
    "X_e['isEstimated'] = 1\n",
    "X_submission['isEstimated'] = 1\n",
    "\n",
    "\n",
    "# Combine\n",
    "X = pd.concat([X_o, X_e])\n",
    "\n",
    "# Add time column that only holds the hour\n",
    "X['time'] = X['date_forecast'].dt.floor('H')\n",
    "X_submission['time'] = X_submission['date_forecast'].dt.floor('H')\n",
    "\n",
    "# combine X and y\n",
    "Xy = pd.merge(X, y, on=['building_id', 'time'], how='inner')\n",
    "\n",
    "# Add monthYear column\n",
    "Xy['monthYear'] = Xy['date_forecast'].dt.to_period('M')\n",
    "X_submission['monthYear'] = X_submission['date_forecast'].dt.to_period('M')\n",
    "\n",
    "# Add dayMonthYear column\n",
    "Xy['dayMonthYear'] = Xy['date_forecast'].dt.to_period('D')\n",
    "X_submission['dayMonthYear'] = X_submission['date_forecast'].dt.to_period('D')\n",
    "\n",
    "# Add month column\n",
    "Xy['month'] = Xy['date_forecast'].dt.month\n",
    "X_submission['month'] = X_submission['date_forecast'].dt.month\n",
    "\n",
    "# Add hour column\n",
    "Xy['hour'] = Xy['date_forecast'].dt.hour\n",
    "X_submission['hour'] = X_submission['date_forecast'].dt.hour\n",
    "\n",
    "# Prepare for joining ->The data is grouped per building and hour\n",
    "Xy = Xy.groupby(['building_id', 'time']).mean().reset_index()\n",
    "X_submission = X_submission.groupby(['building_id', 'time']).mean().reset_index()\n",
    "\n",
    "# Create additional feature for estimated data \"delta_forecast\"\n",
    "Xy['delta_forecast'] = (Xy['time']-Xy['date_calc']).apply(lambda x: x.total_seconds() / 3600)\n",
    "X_submission['delta_forecast'] = (X_submission['time']-X_submission['date_calc']).apply(lambda x: x.total_seconds() / 3600)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 1\n",
    "\n",
    "Xy['GHI'] = Xy['diffuse_rad:W'] + Xy['direct_rad:W']\n",
    "Xy['wind_angle'] = np.arctan2(Xy['wind_speed_u_10m:ms'], Xy['wind_speed_v_10m:ms'])\n",
    "Xy['norm_radiation'] = (Xy['GHI'] / Xy['clear_sky_rad:W']).fillna(0)\n",
    "Xy.loc[Xy['norm_radiation'] > max_val, 'norm_radiation'] = max_val\n",
    "\n",
    "X_submission['GHI'] = X_submission['diffuse_rad:W'] + X_submission['direct_rad:W']\n",
    "X_submission['wind_angle'] = np.arctan2(X_submission['wind_speed_u_10m:ms'], X_submission['wind_speed_v_10m:ms'])\n",
    "X_submission['norm_radiation'] = (X_submission['GHI'] / X_submission['clear_sky_rad:W']).fillna(0)\n",
    "X_submission.loc[X_submission['norm_radiation'] > max_val, 'norm_radiation'] = max_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagged feature of GHI\n",
    "Xy['GHI_lag-2'] = Xy.groupby('building_id')['GHI'].shift(-2)\n",
    "Xy['GHI_lag-1'] = Xy.groupby('building_id')['GHI'].shift(-1)\n",
    "Xy['GHI_lag1'] = Xy.groupby('building_id')['GHI'].shift(1)\n",
    "Xy['GHI_lag2'] = Xy.groupby('building_id')['GHI'].shift(2)\n",
    "\n",
    "X_submission['GHI_lag-2'] = X_submission.groupby('building_id')['GHI'].shift(-2)\n",
    "X_submission['GHI_lag-1'] = X_submission.groupby('building_id')['GHI'].shift(-1)\n",
    "X_submission['GHI_lag1'] = X_submission.groupby('building_id')['GHI'].shift(1)\n",
    "X_submission['GHI_lag2'] = X_submission.groupby('building_id')['GHI'].shift(2)\n",
    "\n",
    "# remove rows were lagged features are nan\n",
    "Xy = Xy.dropna(subset=['GHI_lag-2', 'GHI_lag-1', 'GHI_lag1', 'GHI_lag2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add daily mean of GHI\n",
    "Xy['GHI_mean'] = Xy.groupby(['building_id', 'dayMonthYear'])['GHI'].transform('mean')\n",
    "X_submission['GHI_mean'] = X_submission.groupby(['building_id', 'dayMonthYear'])['GHI'].transform('mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature differences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy['GHI_0_minus_-1'] = Xy['GHI'] - Xy['GHI_lag-1']\n",
    "Xy['GHI_0_minus_-2'] = Xy['GHI'] - Xy['GHI_lag-2']\n",
    "Xy['GHI_0_minus_1'] = Xy['GHI'] - Xy['GHI_lag1']\n",
    "Xy['GHI_0_minus_2'] = Xy['GHI'] - Xy['GHI_lag2']\n",
    "\n",
    "X_submission['GHI_0_minus_-1'] = X_submission['GHI'] - X_submission['GHI_lag-1']\n",
    "X_submission['GHI_0_minus_-2'] = X_submission['GHI'] - X_submission['GHI_lag-2']\n",
    "X_submission['GHI_0_minus_1'] = X_submission['GHI'] - X_submission['GHI_lag1']\n",
    "X_submission['GHI_0_minus_2'] = X_submission['GHI'] - X_submission['GHI_lag2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy['temp*GHI'] = Xy['GHI'] * Xy['t_1000hPa:K']\n",
    "Xy['wind*humidity'] = Xy['wind_speed_10m:ms'] * Xy['relative_humidity_1000hPa:p']\n",
    "Xy['sun_height*diff_rad'] = Xy['sun_elevation:d'] * Xy['diffuse_rad:W']\n",
    "\n",
    "# chat GPT\n",
    "Xy['hour*wind_speed_10m'] = Xy['hour'] * Xy['wind_speed_10m:ms']\n",
    "Xy['hour*clear_sky_rad'] = Xy['hour'] * Xy['clear_sky_rad:W']\n",
    "Xy['month*sun_elevation'] = Xy['month'] * Xy['sun_elevation:d']\n",
    "\n",
    "Xy['relative_humidity*air_density'] = Xy['relative_humidity_1000hPa:p'] * Xy['air_density_2m:kgm3']\n",
    "Xy['temperature*wind_speed'] = Xy['t_1000hPa:K'] * Xy['wind_speed_10m:ms']\n",
    "\n",
    "Xy['GHI*clear_sky_energy'] = Xy['GHI'] * Xy['clear_sky_energy_1h:J']\n",
    "Xy['GHI*sun_azimuth'] = Xy['GHI'] * Xy['sun_azimuth:d']\n",
    "\n",
    "Xy['snow_depth*temp*GHI'] = Xy['snow_depth:cm'] * Xy['temp*GHI']\n",
    "\n",
    "Xy['GHI_lag_interaction'] = Xy['GHI_lag-1'] * Xy['GHI_lag-2'] * Xy['GHI_lag1'] * Xy['GHI_lag2']\n",
    "Xy['GHI_lag_interaction_all'] = Xy['GHI_lag-1'] * Xy['GHI_lag-2'] * Xy['GHI_lag1'] * Xy['GHI_lag2'] * Xy['GHI']\n",
    "\n",
    "Xy['wind_speed*temp*GHI'] = Xy['wind_speed_10m:ms'] * Xy['t_1000hPa:K'] * Xy['GHI']\n",
    "\n",
    "Xy['cloud_base*clear_sky_energy'] = Xy['cloud_base_agl:m'] * Xy['clear_sky_energy_1h:J']\n",
    "\n",
    "Xy['precip*sun_elevation'] = Xy['precip_5min:mm'] * Xy['sun_elevation:d']\n",
    "Xy['supercooled_water*wind_speed'] = Xy['super_cooled_liquid_water:kgm2'] * Xy['wind_speed_10m:ms']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission['temp*GHI'] = X_submission['GHI'] * X_submission['t_1000hPa:K']\n",
    "X_submission['wind*humidity'] = X_submission['wind_speed_10m:ms'] * X_submission['relative_humidity_1000hPa:p']\n",
    "X_submission['sun_height*diff_rad'] = X_submission['sun_elevation:d'] * X_submission['diffuse_rad:W']\n",
    "\n",
    "# chat GPT\n",
    "X_submission['hour*wind_speed_10m'] = X_submission['hour'] * X_submission['wind_speed_10m:ms']\n",
    "X_submission['hour*clear_sky_rad'] = X_submission['hour'] * X_submission['clear_sky_rad:W']\n",
    "X_submission['month*sun_elevation'] = X_submission['month'] * X_submission['sun_elevation:d']\n",
    "\n",
    "X_submission['relative_humidity*air_density'] = X_submission['relative_humidity_1000hPa:p'] * X_submission['air_density_2m:kgm3']\n",
    "X_submission['temperature*wind_speed'] = X_submission['t_1000hPa:K'] * X_submission['wind_speed_10m:ms']\n",
    "\n",
    "X_submission['GHI*clear_sky_energy'] = X_submission['GHI'] * X_submission['clear_sky_energy_1h:J']\n",
    "X_submission['GHI*sun_azimuth'] = X_submission['GHI'] * X_submission['sun_azimuth:d']\n",
    "\n",
    "X_submission['snow_depth*temp*GHI'] = X_submission['snow_depth:cm'] * X_submission['temp*GHI']\n",
    "\n",
    "X_submission['GHI_lag_interaction'] = X_submission['GHI_lag-1'] * X_submission['GHI_lag-2'] * X_submission['GHI_lag1'] * X_submission['GHI_lag2']\n",
    "X_submission['GHI_lag_interaction_all'] = X_submission['GHI_lag-1'] * X_submission['GHI_lag-2'] * X_submission['GHI_lag1'] * X_submission['GHI_lag2'] * X_submission['GHI']\n",
    "\n",
    "X_submission['wind_speed*temp*GHI'] = X_submission['wind_speed_10m:ms'] * X_submission['t_1000hPa:K'] * X_submission['GHI']\n",
    "\n",
    "X_submission['cloud_base*clear_sky_energy'] = X_submission['cloud_base_agl:m'] * X_submission['clear_sky_energy_1h:J']\n",
    "\n",
    "X_submission['precip*sun_elevation'] = X_submission['precip_5min:mm'] * X_submission['sun_elevation:d']\n",
    "X_submission['supercooled_water*wind_speed'] = X_submission['super_cooled_liquid_water:kgm2'] * X_submission['wind_speed_10m:ms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty pv_measurement\n",
    "Xy = Xy.dropna(subset=['pv_measurement'])\n",
    "\n",
    "test_idx = Xy['date_forecast'].between('2021/05/01','2021/07/01') \n",
    "\n",
    "Xy_train = Xy[~test_idx].reset_index(drop=True)\n",
    "Xy_test = Xy[test_idx].reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale y\n",
    "y_scaler = Y_Scaler_MaxAbs_per_building()\n",
    "y_train = y_scaler.fit_transform(y_train, Xy_train['building_id'])\n",
    "#y_o = y_scaler.fit_transform(y_o, X_o['building_id'])\n",
    "#y_e = y_scaler.transform(y_e, X_e['building_id']) # no fit_transform because we use y_e as test data\n",
    "\n",
    "\n",
    "# Scale whole y\n",
    "# full_scaler = RobustScaler()\n",
    "# y_o = full_scaler.fit_transform(y_o.values.reshape(-1, 1)).flatten()\n",
    "# y_e = full_scaler.transform(y_e.values.reshape(-1, 1)).flatten() # no fit_transform because we use y_e as test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X.shape: {Xy_train.shape}\")\n",
    "print(f\"X.shape: {Xy_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "drop_cols = ['time', 'date_forecast', 'snow_density:kgm3',\n",
    "             'date_calc', 'monthYear', 'dayMonthYear']\n",
    "\n",
    "drop2 = ['clear_sky_energy_1h:J',\n",
    "         'direct_rad_1h:J',\n",
    "         'fresh_snow_24h:cm',\n",
    "         'fresh_snow_1h:cm',\n",
    "         'fresh_snow_12h:cm',\n",
    "         'diffuse_rad_1h:J',\n",
    "         'dew_point_2m:K',\n",
    "         'dew_or_rime:idx',\n",
    "         'precip_5min:mm',\n",
    "         'fresh_snow_6h:cm',\n",
    "         'prob_rime:p',\n",
    "         'ceiling_height_agl:m',\n",
    "         'rain_water:kgm2',\n",
    "         'sfc_pressure:hPa',\n",
    "         'snow_depth:cm',\n",
    "         'snow_drift:idx',\n",
    "         'snow_melt_10min:mm',\n",
    "         'snow_water:kgm2',\n",
    "         'pressure_50m:hPa',\n",
    "         'wind_speed_w_1000hPa:ms',\n",
    "         'pressure_100m:hPa',\n",
    "         'fresh_snow_3h:cm']\n",
    "\n",
    "drop_cols = drop_cols  # + drop2\n",
    "\n",
    "print(drop_cols)\n",
    "# ignore if column does not exist\n",
    "X_train = X_train.drop(drop_cols, axis=1, errors='ignore')\n",
    "X_test = X_test.drop(drop_cols, axis=1, errors='ignore')\n",
    "# ignore if column does not exist\n",
    "X = X.drop(drop_cols, axis=1, errors='ignore')\n",
    "X_submission = X_submission.drop(drop_cols, axis=1, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically set types of columns for imputing and oneHotEncoding\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "impute_features = X_train.loc[:, X.isna().any()].columns.tolist()\n",
    "\n",
    "print(f\"categorical_features: {categorical_features}\")\n",
    "print(f\"impute_features: {impute_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty txt file to store status\n",
    "open('status.csv', 'w').close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# BayesSearchCV\n",
    "parameters_bayes = {\n",
    "    'estimator__n_estimators': Integer(30,500),\n",
    "    'estimator__max_depth': Integer(6, 14),\n",
    "    'estimator__learning_rate': Real(0.01, 0.3),\n",
    "    # 'estimator__subsample': Real(0.5, 1.0),\n",
    "    # 'estimator__colsample_bytree': Real(0.8, 1.0),\n",
    "    # 'estimator__colsample_bylevel': Real(0.8, 1.0),\n",
    "    # 'estimator__colsample_bynode': Real(0.8, 1.0),\n",
    "    #'estimator__gamma': Real(0, 2),\n",
    "    'estimator__reg_alpha': Real(0, 10),\n",
    "    'estimator__reg_lambda': Real(1, 10),\n",
    "    'estimator__min_child_weight': Integer(1, 10),\n",
    "    #'estimator__max_delta_step': Integer(0, 5)\n",
    "}\n",
    "\n",
    "parameters_grid = {\n",
    "    'estimator__n_estimators': [20,25,30,35,40,45,50,55,60,65,70,75,100,150,200,250,300],\n",
    "    # 'estimator__max_depth': list(range(3,14)),\n",
    "    # 'estimator__learning_rate': [0.3]#np.arange(0.01, 0.2, 0.01)\n",
    "    # 'estimator__subsample': [0.9,1],\n",
    "    # 'estimator__colsample_bytree': np.arange(0.3, 1, 0.01),\n",
    "    # 'estimator__gamma': [0, 0.5, 1, 1.5, 2],\n",
    "    # 'estimator__reg_alpha': np.arange(0, 10, 0.5),\n",
    "    # 'estimator__reg_lambda': np.arange(0, 10, 0.5),\n",
    "    # 'estimator__min_child_weight': np.arange(0.5, 20, 0.5),\n",
    "    # 'estimator__n_estimators': [75],\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set column transformer\n",
    "columnTransformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('imputer', SimpleImputer(strategy='constant'),impute_features),\n",
    "        ('oneHotEncoder', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "    ],\n",
    "    remainder='passthrough',  # Dont drop remaining columns\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# build the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('columnTransformer', columnTransformer),\n",
    "    ('statusSaver', StatusSaver()),\n",
    "    ('estimator', xgboost.XGBRegressor(\n",
    "        random_state=42,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        reg_alpha=8,\n",
    "        reg_lambda=5,\n",
    "        n_estimators=45,\n",
    "        colsample_bytree=1,\n",
    "        min_child_weight=3,\n",
    "        ))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create bayesian search estimator\n",
    "m1_BayesCV = BayesSearchCV(\n",
    "    pipeline, parameters_bayes, scoring='neg_mean_absolute_error', cv=6, error_score='raise',n_points=6, n_jobs=-1, verbose=2, n_iter=1080, random_state=42)\n",
    "\n",
    "\n",
    "m1_GridCV = GridSearchCV(\n",
    "    pipeline, parameters_grid, scoring='neg_mean_absolute_error', cv=ps, error_score='raise', n_jobs=-1, verbose=2, refit=True)\n",
    "\n",
    "# switch between BayesCV and GridCV\n",
    "m1_CV = m1_GridCV\n",
    "\n",
    "# fit the estimator on the data\n",
    "#m1_CV.fit(X_o, y_o)\n",
    "m1_CV.fit(X_train, y_train)\n",
    "\n",
    "# get best model \n",
    "m1 = m1_CV.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores\n",
    "print('Best score:', m1_CV.best_score_)\n",
    "print('Best parameters:', m1_CV.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot(m1_CV.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key, values in m1_CV.param_grid.items():\n",
    "    pass\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "sns.lineplot(y=m1_CV.cv_results_['mean_test_score'], x=list(map(str, values)))\n",
    "plt.xticks(rotation=45);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m1.steps[-3][1].get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = m1.steps[-3][1].get_feature_names_out()\n",
    "f_importances = m1.steps[-1][1].feature_importances_\n",
    "\n",
    "f_importances_df = pd.DataFrame({'feature': f_names, 'importance': f_importances})\n",
    "\n",
    "f_importances_df = f_importances_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "f_importances_df.plot.bar(x='feature', y='importance', figsize=(20, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(m1_CV) == BayesSearchCV:\n",
    "    _ = plot_objective(m1_CV.optimizer_results_[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on estimated data\n",
    "m1_pred = pd.Series(m1.predict(X_test))\n",
    "t=m1_pred.copy()\n",
    "#m1_pred = pd.Series(full_scaler.inverse_transform(m1_pred.values.reshape(-1, 1)).flatten())\n",
    "m1_pred = y_scaler.inverse_transform(m1_pred, X_test['building_id'])\n",
    "Xy_test['m1_pred'] = m1_pred\n",
    "\n",
    "# calculate abs diff\n",
    "Xy_test['abs_diff'] = np.abs(Xy_test['pv_measurement'] - Xy_test['m1_pred'])\n",
    "Xy_test['diff'] = (Xy_test['pv_measurement'] - Xy_test['m1_pred'])\n",
    "\n",
    "# calculate mae\n",
    "mae = Xy_test['abs_diff'].mean()\n",
    "print('MAE:', mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=Xy_test, x='time', y='pv_measurement', hue='building_id', legend=False)\n",
    "plt.xticks(rotation=90);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=Xy_test, x='time', y='diff', hue='building_id', legend=False)\n",
    "plt.xticks(rotation=90);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the submission file\n",
    "m1.fit(X, y)\n",
    "\n",
    "# prepare dataframes\n",
    "y_test_pred = pd.Series(m1.predict(X_submission))\n",
    "# y_test_pred = pd.Series(full_scaler.inverse_transform(\n",
    "#     y_test_pred.values.reshape(-1, 1)).flatten())\n",
    "#y_test_pred = y_scaler.inverse_transform(y_test_pred, X_t['building_id']).copy()\n",
    "\n",
    "# remove negative predictions\n",
    "y_test_pred.iloc[y_test_pred < 0] = 0\n",
    "\n",
    "# rename columns etc.\n",
    "y_test_pred = y_test_pred.reset_index().rename(\n",
    "    columns={'pv_measurement': 'prediction', 'index': 'id'})\n",
    "\n",
    "# save submission file\n",
    "y_test_pred.to_csv(\n",
    "    'feature_extraction.csv', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-NTNU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
