{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "import xgboost\n",
    "\n",
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding building ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_a['building_id'] = 'a'\n",
    "train_b['building_id'] = 'b'\n",
    "train_c['building_id'] = 'c'\n",
    "\n",
    "X_train_estimated_a['building_id'] = 'a'\n",
    "X_train_estimated_b['building_id'] = 'b'\n",
    "X_train_estimated_c['building_id'] = 'c'\n",
    "\n",
    "X_train_observed_a['building_id'] = 'a'\n",
    "X_train_observed_b['building_id'] = 'b'\n",
    "X_train_observed_c['building_id'] = 'c'\n",
    "\n",
    "X_test_estimated_a['building_id'] = 'a'\n",
    "X_test_estimated_b['building_id'] = 'b'\n",
    "X_test_estimated_c['building_id'] = 'c'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_observed_X = pd.concat([X_train_observed_a,X_train_observed_b,X_train_observed_c])\n",
    "all_estimated_X = pd.concat([X_train_estimated_a,X_train_estimated_b,X_train_estimated_c])\n",
    "\n",
    "X_test = pd.concat([X_test_estimated_a,X_test_estimated_b,X_test_estimated_c])\n",
    "X_train = pd.concat([all_observed_X,all_estimated_X])\n",
    "\n",
    "y_train = pd.concat([train_a,train_b,train_c])\n",
    "\n",
    "X_train['time'] = X_train['date_forecast'].dt.floor('H')\n",
    "X_train_grouped = X_train.groupby(['building_id','time']).mean()\n",
    "\n",
    "X_test['time'] = X_test['date_forecast'].dt.floor('H')\n",
    "X_test_grouped = X_test.groupby(['building_id','time']).mean()\n",
    "\n",
    "data_train = X_train_grouped.merge(y_train,on=['time','building_id'],how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing samples where there is not pv_measurement\n",
    "data_train = data_train.dropna(subset=['pv_measurement'])\n",
    "\n",
    "# DateTime removal\n",
    "#Keeping only month and hour because more relevant for pv_measurement\n",
    "data_train['time_month'] = data_train['time'].dt.month\n",
    "data_train['time_hour'] = data_train['time'].dt.hour\n",
    "\n",
    "X_test = X_test_grouped.reset_index()\n",
    "X_test['time_month'] = X_test['time'].dt.month\n",
    "X_test['time_hour'] = X_test['time'].dt.hour\n",
    "\n",
    "\n",
    "# Splitting observed and estimated (preprocess of date_calc)\n",
    "data_train_o = data_train[data_train['date_calc'].isna()]\n",
    "data_train_e = data_train[data_train['date_calc'].notna()]\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "data_train_e['delta_forecasting'] = (data_train_e['time']-data_train_e['date_calc']).apply(lambda x: x.total_seconds() / 3600)\n",
    "X_test['delta_forecasting'] = (X_test['time']-X_test['date_calc']).apply(lambda x: x.total_seconds() / 3600)\n",
    "\n",
    "data_train_o = data_train_o.sort_values(by='date_forecast')\n",
    "\n",
    "data_train_o.drop(columns=['time','date_forecast','date_calc'],inplace=True)\n",
    "data_train_e.drop(columns=['time','date_forecast','date_calc'],inplace=True)\n",
    "X_test.drop(columns=['time','date_forecast','date_calc'],inplace=True)\n",
    "\n",
    "#Snow density\n",
    "data_train_o.drop('snow_density:kgm3',axis=1, inplace=True)\n",
    "data_train_e.drop('snow_density:kgm3',axis=1, inplace=True)\n",
    "X_test.drop('snow_density:kgm3',axis=1, inplace=True)\n",
    "\n",
    "#Elevation\n",
    "data_train_o.drop('elevation:m',axis=1, inplace=True)\n",
    "data_train_e.drop('elevation:m',axis=1, inplace=True)\n",
    "X_test.drop('elevation:m',axis=1, inplace=True)\n",
    "\n",
    "#Cloud and ceiling\n",
    "data_train_o['cloud_base_agl:m'].fillna(data_train_o['cloud_base_agl:m'].mean(), inplace=True)\n",
    "data_train_o['ceiling_height_agl:m'].fillna(data_train_o['ceiling_height_agl:m'].mean(), inplace=True)\n",
    "\n",
    "data_train_e['cloud_base_agl:m'].fillna(data_train_e['cloud_base_agl:m'].mean(), inplace=True)\n",
    "data_train_e['ceiling_height_agl:m'].fillna(data_train_e['ceiling_height_agl:m'].mean(), inplace=True)\n",
    "\n",
    "X_test['cloud_base_agl:m'].fillna(X_test['cloud_base_agl:m'].mean(), inplace=True)\n",
    "X_test['ceiling_height_agl:m'].fillna(X_test['ceiling_height_agl:m'].mean(), inplace=True)\n",
    "\n",
    "\n",
    "#Resetting indexes\n",
    "data_train_o.reset_index(inplace=True,drop=True)\n",
    "data_train_e.reset_index(inplace=True,drop=True)\n",
    "X_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#One hot encoding\n",
    "#data_train_o = pd.get_dummies(data_train_o, columns=['building_id'], prefix=['id'],dtype=int)\n",
    "#data_train_e = pd.get_dummies(data_train_e, columns=['building_id'], prefix=['id'],dtype=int)\n",
    "#X_test = pd.get_dummies(X_test, columns=['building_id'], prefix=['id'],dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlated Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making assumption only on data_train_o, should be same also for other dataset\n",
    "#corrmat = data_train_o.corr()\n",
    "\n",
    "#groups\n",
    "#groups = []\n",
    "#assigned_features = set()\n",
    "#for col in data_train_o.columns:\n",
    "#    if col not in assigned_features:\n",
    "#        highly_correlated_features = corrmat.index[corrmat[col].abs() > 0.6].tolist()\n",
    "#        highly_correlated_features.remove(col)  # Remove col itself\n",
    "#        if highly_correlated_features:\n",
    "#            highly_correlated_group = [col] + highly_correlated_features\n",
    "#            groups.append(highly_correlated_group)\n",
    "#            assigned_features.update(highly_correlated_group)\n",
    "            #if highly_correlated_group not in groups:\n",
    "            #    groups.append(highly_correlated_group)\n",
    "\n",
    "#for group in groups:\n",
    "#    print(f\"Group: {', '.join(group)}\")\n",
    "\n",
    "#groups that make sense\n",
    "#Humidity Group: absolute_humidity_2m:gm3, air_density_2m:kgm3, dew_point_2m:K, t_1000hPa:K\n",
    "#Clouds height Group: ceiling_height_agl:m, cloud_base_agl:m\n",
    "#Sun Group: clear_sky_energy_1h:J, clear_sky_rad:W, diffuse_rad:W, diffuse_rad_1h:J, direct_rad:W, direct_rad_1h:J, is_day:idx, is_in_shadow:idx, sun_elevation:d\n",
    "#Cloud cover Group: effective_cloud_cover:p, total_cloud_cover:p\n",
    "#Snow Group: fresh_snow_12h:cm, fresh_snow_24h:cm, fresh_snow_3h:cm, fresh_snow_6h:cm, fresh_snow_1h:cm,\n",
    "#Pressure Group: msl_pressure:hPa, pressure_100m:hPa, pressure_50m:hPa, sfc_pressure:hPa\n",
    "\n",
    "#f, ax = plt.subplots(figsize=(12, 9))\n",
    "#sns.heatmap(corrmat, vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA function\n",
    "def pca_analysis(df, features, feature_prefix, n_components, scalers):\n",
    "    df = df.copy(deep=True)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df[features])\n",
    "\n",
    "    pca_transformer = PCA(n_components=n_components)\n",
    "    df_pca = pd.DataFrame(pca_transformer.fit_transform(df_scaled))\n",
    "    df_pca.columns = [feature_prefix + \"_\" + str(i+1) for i in range(n_components)]\n",
    "\n",
    "    df = df.drop(features, axis=1)\n",
    "    #df = pd.concat([df, df_pca], axis=1, ignore_index=True)\n",
    "    df = pd.concat([df, pd.DataFrame(df_pca)], axis=1)\n",
    "    \n",
    "\n",
    "    scalers |= {feature_prefix: {'scaler': scaler, 'pca_transformer': pca_transformer}}\n",
    "\n",
    "    return df, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing PCA on correlated groups\n",
    "# define groups of variables\n",
    "\n",
    "humidity_features_list = ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'dew_point_2m:K', 't_1000hPa:K']\n",
    "\n",
    "cloud_height_features_list = ['ceiling_height_agl:m', 'cloud_base_agl:m']\n",
    "\n",
    "sun_features_list = ['clear_sky_energy_1h:J', 'clear_sky_rad:W', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J', 'is_day:idx', 'is_in_shadow:idx', 'sun_elevation:d']\n",
    "\n",
    "cloud_cover_features_list = ['effective_cloud_cover:p', 'total_cloud_cover:p']\n",
    "\n",
    "snow_features_list = ['fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm']\n",
    "\n",
    "pressure_feature_list = ['msl_pressure:hPa', 'pressure_100m:hPa', 'pressure_50m:hPa', 'sfc_pressure:hPa']\n",
    "\n",
    "time_hour_features_list = ['time_hour', 'sun_azimuth:d']\n",
    "\n",
    "all_groups = [humidity_features_list,cloud_height_features_list,sun_features_list,cloud_cover_features_list,snow_features_list,pressure_feature_list,time_hour_features_list]\n",
    "all_groups_names = ['humidity','cloud_height','sun','cloud_cover','snow','pressure','time_hour']\n",
    "n_pca = [2,1,3,1,2,2,1]\n",
    "\n",
    "data_train_o_pca = data_train_o.copy()\n",
    "scalers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA \n",
    "for group, name, n in zip(all_groups, all_groups_names, n_pca):\n",
    "    data_train_o_pca, scalers = pca_analysis(data_train_o_pca, group, name, n, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corrmat = data_train_o_pca.corr(numeric_only=True)\\nf, ax = plt.subplots(figsize=(12, 9))\\nsns.heatmap(corrmat, vmax=.8, square=True)'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"corrmat = data_train_o_pca.corr(numeric_only=True)\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train_o = data_train_o_pca.copy()\n",
    "#random_forest = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "#X = data_train_o.drop(columns=['pv_measurement'])\n",
    "#y= data_train_o['pv_measurement']\n",
    "\n",
    "#random_forest.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfeature_importance_series = pd.Series(\\n    data=random_forest.feature_importances_,\\n    index=X.columns\\n).sort_values(ascending=False)\\n\\nplt.figure(figsize=(15,5))\\nplt.xticks(rotation=90)\\n\\nsns.barplot(\\n    x=feature_importance_series.index,\\n    y=feature_importance_series.values\\n)'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "feature_importance_series = pd.Series(\n",
    "    data=random_forest.feature_importances_,\n",
    "    index=X.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "sns.barplot(\n",
    "    x=feature_importance_series.index,\n",
    "    y=feature_importance_series.values\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_train_o['pv_measurement']\n",
    "\n",
    "# get y mean per building id\n",
    "mean_y_per_building = y.groupby(data_train_o['building_id']).mean()\n",
    "\n",
    "# divide y by mean per building id\n",
    "y = y.groupby(data_train_o['building_id']).transform(lambda x: x / mean_y_per_building[x.name]) \n",
    "\n",
    "X = data_train_o.drop(columns=['pv_measurement'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=['building_id'], prefix=['id'],dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cristiano\\miniconda3\\envs\\ML-NTNU\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [10:29:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0cec3277c4d9d0165-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"estimator_subsample\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#X_train_xgb = X[:57418]\n",
    "#y_train_xgb = y[:57418]\n",
    "\n",
    "#X_test_xgb = X[57418:]\n",
    "#y_test_xgb = y[57418:]\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "parameters = {\n",
    "    #'estimator__n_estimators': Integer(50, 500),\n",
    "    'estimator__max_depth': [10],\n",
    "    'estimator__learning_rate': [0.01],\n",
    "    'estimator__subsample': [0.9],\n",
    "    'estimator__colsample_bytree': [0.5],\n",
    "    'estimator__gamma': [4.7],\n",
    "    'estimator__reg_alpha': [4.1],\n",
    "    'estimator__reg_lambda': [3],\n",
    "}\n",
    "\n",
    "model1 = xgboost.XGBRegressor(n_estimators = 450,\n",
    "                              max_depth = 10, \n",
    "                              learning_rate = 0.01,\n",
    "                              estimator_subsample = 0.9,\n",
    "                              colsample_bytree = 0.5,\n",
    "                              gamma = 4.7,\n",
    "                              reg_alpha = 4.1,\n",
    "                              reg_lambda = 3)\n",
    "\n",
    "model1.fit(X,y, verbose=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-NTNU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
