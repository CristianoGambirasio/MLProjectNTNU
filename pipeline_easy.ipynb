{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from skopt.space import Integer\n",
    "#!pip install xgboost\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data\n",
    "train_a = pd.read_parquet('A/train_targets.parquet')\n",
    "train_b = pd.read_parquet('B/train_targets.parquet')\n",
    "train_c = pd.read_parquet('C/train_targets.parquet')\n",
    "\n",
    "X_train_estimated_a = pd.read_parquet('A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('C/X_test_estimated.parquet')\n",
    "\n",
    "# Labeling data\n",
    "train_a['building_id'] = 'a'\n",
    "train_b['building_id'] = 'b'\n",
    "train_c['building_id'] = 'c'\n",
    "\n",
    "X_train_estimated_a['building_id'] = 'a'\n",
    "X_train_estimated_b['building_id'] = 'b'\n",
    "X_train_estimated_c['building_id'] = 'c'\n",
    "\n",
    "X_train_observed_a['building_id'] = 'a'\n",
    "X_train_observed_b['building_id'] = 'b'\n",
    "X_train_observed_c['building_id'] = 'c'\n",
    "\n",
    "X_test_estimated_a['building_id'] = 'a'\n",
    "X_test_estimated_b['building_id'] = 'b'\n",
    "X_test_estimated_c['building_id'] = 'c'\n",
    "\n",
    "# Combining data\n",
    "all_observed_X = pd.concat([X_train_observed_a,X_train_observed_b,X_train_observed_c])\n",
    "all_estimated_X = pd.concat([X_train_estimated_a,X_train_estimated_b,X_train_estimated_c])\n",
    "all_y = pd.concat([train_a, train_b, train_c])\n",
    "X_test = pd.concat([X_test_estimated_a,X_test_estimated_b,X_test_estimated_c])\n",
    "\n",
    "# add type of weather data\n",
    "all_observed_X['isObserved'] = 1\n",
    "all_estimated_X['isObserved'] = 0\n",
    "X_test['isObserved'] = 0\n",
    "\n",
    "# Combining training data\n",
    "all_X = pd.concat([all_estimated_X,all_observed_X])\n",
    "\n",
    "# Aggregating all_X\n",
    "all_X_aggregated = all_X.copy()\n",
    "all_X_aggregated['time_hour'] = all_X_aggregated['date_forecast'].dt.floor('H')\n",
    "all_X_aggregated = all_X_aggregated.groupby(['building_id','time_hour','isObserved']).mean().reset_index()\n",
    "\n",
    "# Aggregating X_test\n",
    "X_test_aggregated = X_test.copy()\n",
    "X_test_aggregated['time_hour'] = X_test_aggregated['date_forecast'].dt.floor('H')\n",
    "X_test_aggregated = X_test_aggregated.groupby(['building_id','time_hour','isObserved']).mean().reset_index()\n",
    "\n",
    "# Renaming time column in all_y\n",
    "all_y = all_y.rename(columns={'time': 'time_hour'})\n",
    "\n",
    "# Merging all_X_aggregated and all_y\n",
    "all_train = pd.merge(all_X_aggregated,all_y,on=['building_id','time_hour'],how='right') # right join to keep all y values\n",
    "\n",
    "#Encoding date\n",
    "all_X_aggregated['sin_mon'] = np.sin((all_X_aggregated['time_hour'].dt.month - 1)*np.pi/11)\n",
    "all_X_aggregated['cos_mon'] = np.cos((all_X_aggregated['time_hour'].dt.month - 1)*np.pi/11)\n",
    "\n",
    "all_X_aggregated['sin_hr']= np.sin(all_X_aggregated['time_hour'].dt.hour*np.pi/23)\n",
    "all_X_aggregated['cos_hr']= np.sin(all_X_aggregated['time_hour'].dt.hour*np.pi/23)\n",
    "\n",
    "X_test_aggregated['sin_mon'] = np.sin((X_test_aggregated['time_hour'].dt.month - 1)*np.pi/11)\n",
    "X_test_aggregated['cos_mon'] = np.cos((X_test_aggregated['time_hour'].dt.month - 1)*np.pi/11)\n",
    "\n",
    "X_test_aggregated['sin_hr']= np.sin(X_test_aggregated['time_hour'].dt.hour*np.pi/23)\n",
    "X_test_aggregated['cos_hr']= np.sin(X_test_aggregated['time_hour'].dt.hour*np.pi/23)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define groups of variables\n",
    "\n",
    "sun_features_list = ['clear_sky_energy_1h:J', 'clear_sky_rad:W', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J', 'is_day:idx', 'is_in_shadow:idx', 'sun_elevation:d']\n",
    "\n",
    "humidity_features_list = ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'dew_point_2m:K', 't_1000hPa:K']\n",
    "\n",
    "snow_features_list = ['fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm', 'fresh_snow_3h:cm', 'fresh_snow_6h:cm']\n",
    "\n",
    "cloud_height_features_list = ['ceiling_height_agl:m', 'cloud_base_agl:m']\n",
    "\n",
    "feature_groups = [\n",
    "    {\n",
    "        'name': 'sun',\n",
    "        'features': sun_features_list\n",
    "    },\n",
    "    {\n",
    "        'name': 'humidity',\n",
    "        'features': humidity_features_list\n",
    "    },\n",
    "    {\n",
    "        'name': 'snow',\n",
    "        'features': snow_features_list\n",
    "    },\n",
    "    {\n",
    "        'name': 'cloud_height',\n",
    "        'features': cloud_height_features_list\n",
    "    }\n",
    "]\n",
    "\n",
    "all_pca_features = sun_features_list + humidity_features_list + snow_features_list + cloud_height_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_X_aggregated.merge(all_y,on=['time_hour','building_id'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non useful features\n",
    "drop_features = [\n",
    "    'snow_density:kgm3', # always 250 or nan\n",
    "    'date_calc', # TODO put dates back in, only for testing\n",
    "    'date_forecast', # TODO\n",
    "    'time_hour', # TODO\n",
    "    'elevation:m'\n",
    "]\n",
    "\n",
    "all_data = all_data.drop(columns=drop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill as mean, (iterative imputer)\n",
    "all_data['cloud_base_agl:m'].fillna(all_data['cloud_base_agl:m'].mean(), inplace=True)\n",
    "all_data['ceiling_height_agl:m'].fillna(all_data['ceiling_height_agl:m'].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for test dataset\n",
    "X_test_aggregated = X_test_aggregated.drop(['date_forecast','date_calc','snow_density:kgm3','time_hour','elevation:m'],axis=1) # TODO remove columns at better cell\n",
    "X_test_aggregated['cloud_base_agl:m'].fillna(X_test_aggregated['cloud_base_agl:m'].mean(), inplace=True)\n",
    "X_test_aggregated['ceiling_height_agl:m'].fillna(X_test_aggregated['ceiling_height_agl:m'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    {\n",
    "        \"estimator\": RandomForestRegressor(random_state=42, n_estimators=20, max_depth=20),\n",
    "    },\n",
    "    #{\n",
    "    #    \"estimator\": AdaBoostRegressor(DecisionTreeRegressor(max_depth=10),random_state=42,n_estimators=200),\n",
    "   # },\n",
    "    #{\n",
    "    #    \"estimator\": xgboost.XGBRegressor(n_estimators=200, max_depth= 20),\n",
    "    #},\n",
    "    #{\n",
    "    #    \"estimator\": MLPRegressor(random_state=42)\n",
    "    #},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pca_sun__pca0', 'pca_sun__pca1', 'pca_humidity__pca0',\n",
       "       'pca_humidity__pca1', 'pca_snow__pca0', 'pca_cloud_height__pca0',\n",
       "       'oneHotEncoder__building_id_a', 'oneHotEncoder__building_id_b',\n",
       "       'oneHotEncoder__building_id_c', 'remainder__isObserved',\n",
       "       'remainder__dew_or_rime:idx', 'remainder__effective_cloud_cover:p',\n",
       "       'remainder__msl_pressure:hPa', 'remainder__precip_5min:mm',\n",
       "       'remainder__precip_type_5min:idx', 'remainder__pressure_100m:hPa',\n",
       "       'remainder__pressure_50m:hPa', 'remainder__prob_rime:p',\n",
       "       'remainder__rain_water:kgm2',\n",
       "       'remainder__relative_humidity_1000hPa:p',\n",
       "       'remainder__sfc_pressure:hPa', 'remainder__snow_depth:cm',\n",
       "       'remainder__snow_drift:idx', 'remainder__snow_melt_10min:mm',\n",
       "       'remainder__snow_water:kgm2', 'remainder__sun_azimuth:d',\n",
       "       'remainder__super_cooled_liquid_water:kgm2',\n",
       "       'remainder__total_cloud_cover:p', 'remainder__visibility:m',\n",
       "       'remainder__wind_speed_10m:ms', 'remainder__wind_speed_u_10m:ms',\n",
       "       'remainder__wind_speed_v_10m:ms',\n",
       "       'remainder__wind_speed_w_1000hPa:ms', 'remainder__sin_mon',\n",
       "       'remainder__cos_mon', 'remainder__sin_hr', 'remainder__cos_hr'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows that are NaN in target column\n",
    "all_data = all_data[~all_data['pv_measurement'].isna()]\n",
    "\n",
    "# shuffle all_data to have approximately the same distribution of buildings and observed/estimated in each fold of CV\n",
    "all_data = all_data.sample(frac=1, random_state=42).reset_index(drop=True) # TODO turn back on?\n",
    "\n",
    "X_test = X_test_aggregated\n",
    "\n",
    "# define X, y and X_test\n",
    "X = all_data.drop(['pv_measurement'], axis=1)\n",
    "y = all_data['pv_measurement']\n",
    "\n",
    "sun_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2))\n",
    "])\n",
    "humidity_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca' , PCA(n_components=2))\n",
    "])\n",
    "snow_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=1))\n",
    "])\n",
    "cloud_height_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=1))\n",
    "])\n",
    "\n",
    "# set column transformer\n",
    "columnTransformer = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('pca_sun', sun_pipeline, sun_features_list),\n",
    "        ('pca_humidity', humidity_pipeline, humidity_features_list),\n",
    "        ('pca_snow', snow_pipeline, snow_features_list),\n",
    "        ('pca_cloud_height', cloud_height_pipeline, cloud_height_features_list),\n",
    "        ('oneHotEncoder', OneHotEncoder(handle_unknown='ignore'), ['building_id']),\n",
    "    ],\n",
    "    remainder='passthrough',  # Include other columns\n",
    ")\n",
    "\n",
    "# build the pipeline\n",
    "pipelinePrep = Pipeline([\n",
    "    ('columnTransformer', columnTransformer),\n",
    "])\n",
    "\n",
    "prep_model = pipelinePrep.fit(X)\n",
    "\n",
    "X = pipelinePrep.fit_transform(X)\n",
    "X_test = pipelinePrep.fit_transform(X_test)\n",
    "prep_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cutoff = int(len(X)*0.8)\\n\\nX_train = X[:cutoff]\\nX_val = X[cutoff:]\\ny_train = y[:cutoff]\\ny_val = y[cutoff:]\\n\\nmodels = []\\n\\n# main training function\\nfor estimator in estimators:\\n  # set pca pipelines\\n  # build the pipeline\\n    pipeline_model = Pipeline([\\n        (\\'estimator\\', estimator.get(\"estimator\"))\\n    ])\\n\\n    model = pipeline_model.fit(X_train,y_train)\\n    models.append(model)'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"cutoff = int(len(X)*0.8)\n",
    "\n",
    "X_train = X[:cutoff]\n",
    "X_val = X[cutoff:]\n",
    "y_train = y[:cutoff]\n",
    "y_val = y[cutoff:]\n",
    "\n",
    "models = []\n",
    "\n",
    "# main training function\n",
    "for estimator in estimators:\n",
    "  # set pca pipelines\n",
    "  # build the pipeline\n",
    "    pipeline_model = Pipeline([\n",
    "        ('estimator', estimator.get(\"estimator\"))\n",
    "    ])\n",
    "\n",
    "    model = pipeline_model.fit(X_train,y_train)\n",
    "    models.append(model)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    {\n",
    "        \"estimator\": RandomForestRegressor(random_state=42),\n",
    "        \"parameters\": {\n",
    "            'estimator__n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "            'estimator__max_depth': Integer(1, 32),\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"estimator\": AdaBoostRegressor(DecisionTreeRegressor(),random_state=42),\n",
    "        \"parameters\": {\n",
    "            'estimator__n_estimators': [500, 1000, 2000],\n",
    "            'learning_rate':[.001,0.01,.1],\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"estimator\": xgboost.XGBRegressor(),\n",
    "        \"parameters\": {\n",
    "            'estimator__n_estimators': Integer(10, 500),\n",
    "            'estimator__max_depth': Integer(2, 10),\n",
    "            'estimator__eta': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "            'estimator__min_child_weight': [1,5,20,200],\n",
    "            'estimator__max_depth': Integer(3,10)\n",
    "\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"estimator\": MLPRegressor(random_state=42),\n",
    "        \"parameters\": {\n",
    "            'estimator__max_iter': Integer(10, 1000),\n",
    "            #'estimator__hidden_layer_sizes': [(150,100,50), (120,80,40), (100,50,30)], when knowing n_layers\n",
    "            'estimator__n_layers': Integer(2, 6),\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "models_tuned = []\n",
    "for estimator in estimators:\n",
    "  # set pca pipelines\n",
    "  # build the pipeline\n",
    "    pipeline_model = Pipeline([\n",
    "        ('estimator', estimator.get(\"estimator\"))\n",
    "    ])\n",
    "\n",
    "    parameters = estimator.get(\"parameters\") \n",
    "\n",
    "    bayes_search_estimator = BayesSearchCV(\n",
    "        pipeline_model, parameters, scoring='neg_mean_absolute_error', cv=3, error_score='raise', n_jobs=-1, verbose=10, n_iter=5, random_state=42)\n",
    "    \n",
    "    bayes_search_estimator.fit(X, y)\n",
    "    \n",
    "    models_tuned.append(bayes_search_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number = 0\n",
    "with open('readme.txt', 'w') as f:\n",
    "    for model in models_tuned:\n",
    "        f.write(\"\\nMODEL \"+str(models_tuned[model_number].best_estimator_.named_steps)+\"\\n\")\n",
    "        f.write(\"best params: \"+str(models_tuned[model_number].best_params_)+\"\\n\")\n",
    "        f.write(\"best score \"+str(models_tuned[model_number].best_score_)+\"\\n\")\n",
    "        model_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.57987733486269"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_0 = models[0].predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred_0)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Cristiano\\Documents\\GitHub\\MLProjectNTNU\\pipeline_easy.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Cristiano/Documents/GitHub/MLProjectNTNU/pipeline_easy.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_pred_1 \u001b[39m=\u001b[39m models[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mpredict(X_val)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cristiano/Documents/GitHub/MLProjectNTNU/pipeline_easy.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mae \u001b[39m=\u001b[39m mean_absolute_error(y_val, y_pred_1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cristiano/Documents/GitHub/MLProjectNTNU/pipeline_easy.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mae\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "y_pred_1 = models[1].predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred_1)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146.378556846439"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_2 = models[2].predict(X_val)\n",
    "\n",
    "mae = mean_absolute_error(y_val, y_pred_2)\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Cristiano\\Documents\\GitHub\\MLProjectNTNU\\pipeline_easy.ipynb Cell 24\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cristiano/Documents/GitHub/MLProjectNTNU/pipeline_easy.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cristiano/Documents/GitHub/MLProjectNTNU/pipeline_easy.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#plt.plot(y_pred_0, color='blue')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cristiano/Documents/GitHub/MLProjectNTNU/pipeline_easy.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#plt.plot(y_pred_1, color='red')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cristiano/Documents/GitHub/MLProjectNTNU/pipeline_easy.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#plt.plot(np.array(y_val), color='green')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cristiano/Documents/GitHub/MLProjectNTNU/pipeline_easy.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#plt.plot(y_pred_2, color='cyan')\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Cristiano/Documents/GitHub/MLProjectNTNU/pipeline_easy.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marray(y_val)\u001b[39m-\u001b[39my_pred_2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_2' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "#plt.plot(y_pred_0, color='blue')\n",
    "#plt.plot(y_pred_1, color='red')\n",
    "#plt.plot(np.array(y_val), color='green')\n",
    "#plt.plot(y_pred_2, color='cyan')\n",
    "\n",
    "plt.plot(np.array(y_val)-y_pred_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test['prediction'] = models[-1].predict(X_test)\n",
    "sample_submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "sample_submission.to_csv('nn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-NTNU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
